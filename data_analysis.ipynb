{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries required.\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from umap import UMAP\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data and inspect it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from the CSV file\n",
    "study_data= pd.read_csv('data/mental-heath-in-tech-2016_20161114.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the columns in the dataset\n",
    "for col in study_data.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shorten column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the original colums to compare later\n",
    "orig_columns = study_data.columns\n",
    "\n",
    "# Rename the columns for better readability\n",
    "study_data.columns = [\n",
    "    'self_employed',\n",
    "    'num_employees',\n",
    "    'employer_tech_company',\n",
    "    'role_related_to_tech',\n",
    "    'mental_health_benefits',\n",
    "    'know_mental_health_options',\n",
    "    'employer_discussed_mh',\n",
    "    'employer_mh_resources',\n",
    "    'anonymity_protected',\n",
    "    'mh_leave_comfort',\n",
    "    'mh_discussion_negative',\n",
    "    'ph_discussion_negative',\n",
    "    'mh_comfort_coworkers',\n",
    "    'mh_comfort_supervisor',\n",
    "    'employer_mh_priority',\n",
    "    'negative_consequences_observed',\n",
    "    'medical_coverage_mh',\n",
    "    'know_local_mh_resources',\n",
    "    'reveal_mh_clients',\n",
    "    'reveal_mh_clients_negative',\n",
    "    'reveal_mh_coworkers',\n",
    "    'reveal_mh_coworkers_negative',\n",
    "    'productivity_affected',\n",
    "    'work_time_affected_pct',\n",
    "    'previous_employers',\n",
    "    'prev_employers_mh_benefits',\n",
    "    'prev_employers_mh_options',\n",
    "    'prev_employers_discussed_mh',\n",
    "    'prev_employers_mh_resources',\n",
    "    'prev_employers_anonymity',\n",
    "    'prev_employers_mh_negative',\n",
    "    'prev_employers_ph_negative',\n",
    "    'mh_comfort_prev_coworkers',\n",
    "    'mh_comfort_prev_supervisor',\n",
    "    'prev_employers_mh_priority',\n",
    "    'prev_employers_negative_obs',\n",
    "    'physical_health_in_interview',\n",
    "    'physical_health_in_interview_reason',\n",
    "    'mental_health_in_interview',\n",
    "    'mental_health_in_interview_reason',\n",
    "    'mh_hurt_career',\n",
    "    'mh_viewed_negatively',\n",
    "    'mh_share_friends_family',\n",
    "    'unsupportive_response',\n",
    "    'observed_mh_discussion_effect',\n",
    "    'family_history_mh',\n",
    "    'past_mh_disorder',\n",
    "    'current_mh_disorder',\n",
    "    'current_mh_condition',\n",
    "    'maybe_mh_condition',\n",
    "    'diagnosed_mh_condition',\n",
    "    'diagnosed_mh_condition_details',\n",
    "    'sought_mh_treatment',\n",
    "    'mh_treatment_effective',\n",
    "    'mh_treatment_ineffective',\n",
    "    'age',\n",
    "    'gender',\n",
    "    'country_residence',\n",
    "    'us_state_residence',\n",
    "    'country_work',\n",
    "    'us_state_work',\n",
    "    'work_position',\n",
    "    'work_remote'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the original columns with the new columns\n",
    "for i in range(len(orig_columns)):\n",
    "    print(study_data.columns[i], ' <- ', orig_columns[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a brief overview of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display main statistics of each column\n",
    "describe = study_data.describe(include='all').T.to_string()\n",
    "print(describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all unique values of each column\n",
    "for col in study_data.columns:\n",
    "    print('\\n',study_data[col].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove answers with no informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing uncertain answers  \n",
    "study_data = study_data.replace('Not applicable to me',np.nan)\n",
    "study_data = study_data.replace('I don\\'t know',np.nan)\n",
    "study_data = study_data.replace('I\\'m not sure',np.nan)\n",
    "study_data = study_data.replace('N/A (not currently aware)',np.nan)\n",
    "study_data = study_data.replace('Not eligible for coverage / N/A',np.nan)\n",
    "study_data = study_data.replace('Not applicable to me (I do not have a mental illness)', np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unify country names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping for replacement\n",
    "country_mapping = {\n",
    "    'United States of America': 'USA',\n",
    "    'United Kingdom': 'UK'\n",
    "}\n",
    "\n",
    "# Replace values in both 'country_live' and 'country_work' columns\n",
    "study_data[['country_residence', 'country_work']] = study_data[['country_residence', 'country_work']].replace(country_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop redundant and unecessary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check where county_work is different from country_residence\n",
    "i = 0\n",
    "for index, row in study_data.iterrows():\n",
    "    if row['country_residence'] != row['country_work']:\n",
    "        print(row['country_residence'], ' -> ', row['country_work'])\n",
    "        i += 1\n",
    "\n",
    "print('\\nTotal number of people having different work than residence countries: ', i/index*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Drop country_residence, us_state_residence, us_state_work because the information is not relevant for the topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_data.drop(columns=['country_residence'], inplace=True)    # Drop the country_residence column\n",
    "study_data.drop(columns=['us_state_residence'], inplace=True)   # Drop the us_state_residence column\n",
    "study_data.drop(columns=['us_state_work'], inplace=True)        # Drop the us_state_work column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find and drop coulums with contextual answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find columns with strings greater than 10 characters\n",
    "for col in study_data.columns:\n",
    "    if study_data[col].dtype == 'object':\n",
    "        if study_data[col].str.len().max() > 10:\n",
    "            print('\\n',study_data[col].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns with contextual data\n",
    "study_data.drop(columns=['physical_health_in_interview_reason'], inplace=True)\n",
    "study_data.drop(columns=['mental_health_in_interview_reason'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace age outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sort(study_data['age'].unique()))       # Display unique age values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_copy = study_data['age'].copy()                     # Create a copy for later comparison\n",
    "study_data['age'] = study_data['age'].apply(\n",
    "    lambda x: np.nan if x < 18 or x > 75 else x         # Replace age outliers with NaN\n",
    "    )\n",
    "age_checksum = (study_data['age'] != age_copy).sum()    # Check how many rows were changed\n",
    "\n",
    "print(f'Replaced age outliers with NaN for {age_checksum} rows.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill missing age values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_copy_2 = study_data['age'].copy()                       # Create a copy for later comparison\n",
    "age_median = study_data['age'].median()                     # Calculate the median age\n",
    "study_data.fillna({'age': age_median}, inplace=True)        # Fill missing age values with the median\n",
    "age_checksum_2 = (age_copy_2 != study_data['age']).sum()    # Check how many rows were changed\n",
    "\n",
    "print(f'Filled missing age values with the median: {age_median}  for {age_checksum_2} rows.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce age to categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_data['age'] = pd.cut(\n",
    "    study_data['age'], \n",
    "    bins=[18, 30, 40, 50, 60, float('inf')],  # Extend bins to include all ages above 60\n",
    "    labels=['18-30', '30-40', '40-50', '50-60', '60+'], \n",
    "    right=False\n",
    ")\n",
    "print(study_data['age'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace company size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_data['num_employees'] = study_data['num_employees'].replace('More than 1000', '1000+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop rows with too many missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_count = study_data.shape[0]                                            # Get the original row count\n",
    "study_data = study_data.dropna(thresh=study_data.shape[1] * 0.65)               # Drop rows with over 35% missing values\n",
    "drop_lines_percent = (1 - (study_data.shape[0] / original_count)) *100          # Calculate the percentage of dropped rows\n",
    "\n",
    "print(f'Dropped {drop_lines_percent}% of rows due to missing values.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop columns with to many missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get an overview of the nan percentage in the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate NaN percentages\n",
    "nan_percentage = (study_data.isna().sum() / len(study_data)) * 100\n",
    "# Sort by percentage in descending order\n",
    "nan_percentage_sorted = nan_percentage.sort_values(ascending=False)\n",
    "\n",
    "# Display the sorted percentages\n",
    "print(nan_percentage_sorted.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop columns with more than 50% of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name, percentage in nan_percentage_sorted.items():         # Iterate over the sorted percentages \n",
    "    if percentage > 50:                                            # If the percentage is over 30%\n",
    "        study_data.drop(col_name, axis=1, inplace=True)            # Drop the column from study_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for missing values percentage again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_percentage = (study_data.isna().sum() / len(study_data)) * 100  # Recalculate NaN percentages\n",
    "nan_percentage_sorted = nan_percentage.sort_values(ascending=False) # Sort by percentage in descending order\n",
    "\n",
    "print(nan_percentage_sorted.to_string())                            # Display the sorted percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define gender categories with a dictionary for efficient replacement\n",
    "gender_map = {\n",
    "    **{key: 'male' for key in ['Male', 'male', 'Male ', 'M', 'm', 'man', 'Cis male', 'Male.', \n",
    "                               'male 9:1 female, roughly', 'Male (cis)', 'Man', 'Sex is male',\n",
    "                               'cis male', 'Malr', 'Dude', \n",
    "                               'I\\'m a man why didn\\'t you make this a drop down question. You should of asked sex? And I would of answered yes please. Seriously how much text can this take? ',\n",
    "                               'mail', 'M|', 'Male/genderqueer', 'male ', 'Cis Male', \n",
    "                               'Male (trans, FtM)', 'cisdude', 'cis man', 'MALE']},\n",
    "                               \n",
    "    **{key: 'female' for key in ['Female', 'female', 'I identify as female.', 'female ', \n",
    "                                 'Female assigned at birth ',\n",
    "                                 'F', 'Woman', 'fm', 'f', 'Cis female ', 'Transitioned, M2F',\n",
    "                                 'Genderfluid (born female)', 'Female or Multi-Gender Femme', \n",
    "                                 'Female ', 'woman', 'female/woman', 'Cisgender Female', 'fem', \n",
    "                                 'Female (props for making this a freeform field, though)', \n",
    "                                 ' Female', 'Cis-woman', 'female-bodied; no feelings about gender',\n",
    "                                 'AFAB']}\n",
    "}\n",
    "\n",
    "# Store initial gender value counts\n",
    "gender_before = study_data['gender'].value_counts()\n",
    "\n",
    "# Replace genders using the map and set all non-male/female values to 1\n",
    "study_data['gender'] = study_data['gender'].replace(gender_map).apply(\n",
    "    lambda x: 'queer' if x not in ['male', 'female'] else x\n",
    "    )\n",
    "\n",
    "# Display results\n",
    "print('Before:\\n', gender_before, '\\n\\n\\nAfter:\\n', study_data['gender'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace answers in 'mh_hurt_career', 'mh_viewed_negatively', 'mh_share_friends_family','unsupportive_response'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define gender categories with a dictionary for efficient replacement\n",
    "answer_map = {\n",
    "    **{key: 'Yes' for key in ['Yes, I think it would', 'Yes, it has', 'Yes, I think they would',\n",
    "                                'Yes, they do', 'Somewhat open', 'Very open',\n",
    "                                'Yes', 'Yes, I observed', 'Yes, I experienced']},\n",
    "                               \n",
    "    **{key: 'Maybe' for key in ['Maybe', 'Neutral', 'Maybe/Not sure']},\n",
    "\n",
    "    **{key: 'No' for key in ['No, it has not', 'No, I don\\'t think it would',\n",
    "                             'No, they do not','No, I don\\'t think they would',\n",
    "                             'Not open at all', 'Somewhat not open']}\n",
    "}\n",
    "\n",
    "# Make a list of columns to replace\n",
    "columns_to_replace = ['mh_hurt_career', 'mh_viewed_negatively', 'mh_share_friends_family', 'unsupportive_response']\n",
    "\n",
    "# Replace genders using the map and set all non-male/female values to 1\n",
    "for column in columns_to_replace:\n",
    "    study_data[column] = study_data[column].replace(answer_map)\n",
    "\n",
    "# Display results\n",
    "for column in columns_to_replace:\n",
    "    print(study_data[column].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing countries of work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique values of the 'country_work' column\n",
    "print(study_data['country_work'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the continent mappings\n",
    "continent_mapping = {\n",
    "    **{country: 'north_america' for country in ['United States of America', 'Canada', 'Mexico']},\n",
    "    **{country: 'europe' for country in [\n",
    "        'United Kingdom', 'Netherlands', 'Germany', 'Sweden', 'France',\n",
    "        'Ireland', 'Switzerland', 'Bulgaria', 'Finland', 'Denmark',\n",
    "        'Russia', 'Spain', 'Norway', 'Austria', 'Bosnia and Herzegovina',\n",
    "        'Italy', 'Poland', 'Belgium', 'Czech Republic']},\n",
    "}\n",
    "\n",
    "# Replace the values in 'country_work' column with the mapped values or 'Other'\n",
    "study_data['country_work'] = study_data['country_work'].apply(lambda x: continent_mapping.get(x, 'Other'))\n",
    "\n",
    "# Display the unique values of the 'country_work' column\n",
    "print(study_data['country_work'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing work position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique values of the 'work_position' column\n",
    "print(study_data['work_position'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized category mapping with direct keyword-to-category association\n",
    "keyword_to_category = {\n",
    "    'supervisor': 'Management',\n",
    "    'team lead': 'Management',\n",
    "    'leadership': 'Management',\n",
    "    'executive': 'Management',\n",
    "    'back': 'Development',\n",
    "    'developer': 'Development',\n",
    "    'front': 'Development',\n",
    "    'devops': 'Development',\n",
    "    'sysadmin': 'Development',\n",
    "    'dev': 'Development',\n",
    "}\n",
    "\n",
    "# Function to classify roles\n",
    "def categorize_role_optimized(role):\n",
    "    role = role.lower()  # Standardize to lowercase\n",
    "    for keyword, category in keyword_to_category.items():\n",
    "        if keyword in role:\n",
    "            return category\n",
    "    return 'Other'  # Default for unmatched roles\n",
    "\n",
    "# Convert the entire column to lowercase for efficiency\n",
    "study_data['work_position'] = study_data['work_position'].str.lower()\n",
    "\n",
    "# Apply the optimized categorization function\n",
    "study_data['work_position'] = study_data['work_position'].apply(categorize_role_optimized)\n",
    "\n",
    "# Display the unique values and their counts\n",
    "print(study_data['work_position'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display all unique values for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in study_data.columns:\n",
    "    print('\\n\\n', study_data[col].value_counts().to_string(), '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the  diagnosed_mh_condition_details column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set to store the individual values\n",
    "unique_values = set()\n",
    "\n",
    "# Iterate over the rows and save the unique strings split at '|'\n",
    "for value in study_data['diagnosed_mh_condition_details']:\n",
    "    if pd.notna(value):\n",
    "        unique_values.update(value.split('|'))\n",
    "\n",
    "# Display the unique values\n",
    "print('\\n'.join(unique_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the rows and split at '|'\n",
    "study_data['diagnosed_mh_condition_details'] = study_data['diagnosed_mh_condition_details'].apply(\n",
    "    lambda x: x.split('|') if pd.notna(x) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping of disorders to categories\n",
    "disorder_category_mapping = {\n",
    "    # Mood Disorders\n",
    "    'Depression': 'Mood Disorders',\n",
    "    'Seasonal Affective Disorder': 'Mood Disorders',\n",
    "    'Mood Disorder (Depression, Bipolar Disorder, etc)': 'Mood Disorders',\n",
    "    'Suicidal Ideation': 'Mood Disorders',\n",
    "    'Intimate Disorder': 'Mood Disorders',\n",
    "\n",
    "    # Anxiety and Stress-Related Disorders\n",
    "    'Anxiety Disorder (Generalized, Social, Phobia, etc)': 'Anxiety and Stress-Related Disorders',\n",
    "    'Post-traumatic Stress Disorder': 'Anxiety and Stress-Related Disorders',\n",
    "    'Stress Response Syndromes': 'Anxiety and Stress-Related Disorders',\n",
    "    'Obsessive-Compulsive Disorder': 'Anxiety and Stress-Related Disorders',\n",
    "    'Gender Identity Disorder': 'Anxiety and Stress-Related Disorders',\n",
    "     'posttraumatic stress disourder': 'Anxiety and Stress-Related Disorders',\n",
    "\n",
    "    # Other\n",
    "    'Attention Deficit Hyperactivity Disorder': 'Other',\n",
    "    'ADD (w/o Hyperactivity)': 'Other',\n",
    "    'Attention Deficit Disorder': 'Other',\n",
    "    'MCD (when it was diagnosed, the ultra-mega \\'disorder\\' ADHD didn\\'t exist yet)': 'Other',\n",
    "    'Autism Spectrum Disorder': 'Other',\n",
    "    'Autism': 'Other',\n",
    "    'Autism - while not a \\'mental illness\\', still greatly affects how I handle anxiety': 'Other',\n",
    "    'Asperger Syndrome': 'Other',\n",
    "    'Aspergers': 'Other',\n",
    "    'Personality Disorder (Borderline, Antisocial, Paranoid, etc)': 'Other',\n",
    "    'Gender Dysphoria': 'Other',\n",
    "    'Eating Disorder (Anorexia, Bulimia, etc)': 'Other',\n",
    "    'Psychotic Disorder (Schizophrenia, Schizoaffective, etc)': 'Other',\n",
    "    'Dissociative Disorder': 'Other',\n",
    "    'Substance Use Disorder': 'Other',\n",
    "    'Addictive Disorder': 'Other',\n",
    "}\n",
    "\n",
    "# Function to map disorders to categories\n",
    "def map_disorders_to_categories(disorder_list):\n",
    "    if isinstance(disorder_list, list):  # Check if the input is a list\n",
    "        mapped_categories = []  # Initialize an empty list\n",
    "        for item in disorder_list:  # Iterate over the list\n",
    "            mapped_categories.append(disorder_category_mapping.get(item, 'Other'))  # Map the disorder to a category\n",
    "        return mapped_categories    # Return the list of categories\n",
    "    else:\n",
    "        return disorder_list  # Return as-is if not a list\n",
    "    \n",
    "# Apply the mapping to the DataFrame\n",
    "study_data['diagnosed_mh_condition_details'] = study_data['diagnosed_mh_condition_details'].apply(map_disorders_to_categories)\n",
    "\n",
    "# Take the mode of each row in the 'diagnosed_mh_condition_details' column\n",
    "study_data['diagnosed_mh_condition_details'] = study_data['diagnosed_mh_condition_details'].apply(\n",
    "    lambda x: max(set(x), key=x.count) if isinstance(x, list) else x\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique values in the column\n",
    "print(study_data['diagnosed_mh_condition_details'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get an overview of the missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the missing value counts for each column and the most frequent value\n",
    "for col in study_data.columns:\n",
    "    print(f'\\n{col}:')\n",
    "    print('Number of missing values: ', study_data[col].isna().sum())\n",
    "    print('Most frequent value:      ', study_data[col].mode().values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use resample to impute the missing data based on the proportion of the existing data to preserve variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the columns and replace missing values with the most frequent value\n",
    "for col in study_data.columns:\n",
    "  \n",
    "    # Replace missing values statistical most likely value\n",
    "    missing = study_data[col].isnull()\n",
    "    probs = study_data[col].value_counts(normalize=True)\n",
    "    study_data.loc[missing, col] = np.random.choice(probs.index, size=len(study_data[missing]), p=probs.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if inputing was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a check variable\n",
    "check = True\n",
    "\n",
    "# Check if there are any missing values left\n",
    "for col in study_data.columns:\n",
    "    if study_data[col].isna().sum() > 0:\n",
    "        print(col)\n",
    "    else:\n",
    "        check = False\n",
    "\n",
    "# If there are no missing values left, display a message\n",
    "if not check:\n",
    "    print('No missing values left.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final data shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the final data set to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to a new CSV file\n",
    "study_data.to_csv('data/prep_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define column encoding types and ordinal mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns based on types\n",
    "numerical_cols = [\n",
    "    'employer_tech_company', \n",
    "    'self_employed', \n",
    "    'sought_mh_treatment', \n",
    "    'previous_employers'\n",
    "]\n",
    "\n",
    "# All columns with defined ordinal mapping\n",
    "ordinal_cols = [\n",
    "    'num_employees',\n",
    "    'mental_health_benefits',\n",
    "    'know_mental_health_options',\n",
    "    'employer_discussed_mh',\n",
    "    'employer_mh_resources',\n",
    "    'mh_leave_comfort',\n",
    "    'mh_discussion_negative',\n",
    "    'ph_discussion_negative',\n",
    "    'mh_comfort_coworkers',\n",
    "    'mh_comfort_supervisor',\n",
    "    'employer_mh_priority',\n",
    "    'negative_consequences_observed',\n",
    "    'prev_employers_mh_benefits',\n",
    "    'prev_employers_mh_options',\n",
    "    'prev_employers_discussed_mh',\n",
    "    'prev_employers_mh_resources',\n",
    "    'prev_employers_mh_negative',\n",
    "    'prev_employers_ph_negative',\n",
    "    'mh_comfort_prev_coworkers',\n",
    "    'mh_comfort_prev_supervisor',\n",
    "    'prev_employers_mh_priority',\n",
    "    'prev_employers_negative_obs',\n",
    "    'physical_health_in_interview',\n",
    "    'mental_health_in_interview',\n",
    "    'mh_hurt_career',\n",
    "    'mh_viewed_negatively',\n",
    "    'mh_share_friends_family',\n",
    "    'unsupportive_response',\n",
    "    'observed_mh_discussion_effect',\n",
    "    'family_history_mh',\n",
    "    'past_mh_disorder',\n",
    "    'current_mh_disorder',\n",
    "    'diagnosed_mh_condition',\n",
    "    'mh_treatment_effective',\n",
    "    'mh_treatment_ineffective',\n",
    "    'age',\n",
    "    'gender',\n",
    "    'work_remote'\n",
    "]\n",
    "\n",
    "# Remaining categorical columns (excluding ordinal columns)\n",
    "categorical_cols = [\n",
    "    'country_work', \n",
    "    'work_position',\n",
    "    'diagnosed_mh_condition_details'\n",
    "]\n",
    "\n",
    "# Define the column transformer\n",
    "ordinal_mapping = {\n",
    "    'num_employees': ['1-5', '6-25', '26-100', '100-500', '500-1000', '1000+'],\n",
    "    'mental_health_benefits': ['No', 'Yes'],\n",
    "    'know_mental_health_options': ['No', 'I am not sure', 'Yes'],\n",
    "    'employer_discussed_mh': ['No', 'Maybe', 'Yes'],\n",
    "    'employer_mh_resources': ['No', 'Some', 'Yes'],\n",
    "    'mh_leave_comfort': ['Very difficult', 'Somewhat difficult',  'Neither easy nor difficult', 'Somewhat easy', 'Very easy'],\n",
    "    'mh_discussion_negative': ['No', 'Maybe', 'Yes'],\n",
    "    'ph_discussion_negative': ['No', 'Maybe', 'Yes'],\n",
    "    'mh_comfort_coworkers': ['No', 'Maybe', 'Yes'],\n",
    "    'mh_comfort_supervisor': ['No', 'Maybe', 'Yes'],\n",
    "    'employer_mh_priority': ['No', 'Somewhat', 'Yes'],\n",
    "    'negative_consequences_observed': ['No', 'Maybe', 'Yes'],\n",
    "    'prev_employers_mh_benefits': ['No, none did', 'Some did', 'Yes, they all did'],\n",
    "    'prev_employers_mh_options': ['No, I only became aware later', 'I was aware of some', 'Yes, I was aware of all of them'],\n",
    "    'prev_employers_discussed_mh': ['None did', 'Some did', 'Yes, they all did'],\n",
    "    'prev_employers_mh_resources': ['None did', 'Some did', 'Yes, they all did'],\n",
    "    'prev_employers_mh_negative': ['None of them', 'Some of them', 'Yes, all of them'],\n",
    "    'prev_employers_ph_negative': ['None of them', 'Some of them', 'Yes, all of them'],\n",
    "    'mh_comfort_prev_coworkers': ['No, at none of my previous employers', 'Some of my previous employers', 'Yes, at all of my previous employers'],\n",
    "    'mh_comfort_prev_supervisor': ['No, at none of my previous employers', 'Some of my previous employers', 'Yes, at all of my previous employers'],\n",
    "    'prev_employers_mh_priority': ['None did', 'Some did', 'Yes, they all did'],\n",
    "    'prev_employers_negative_obs': ['None of them', 'Some of them', 'Yes, all of them'],\n",
    "    'physical_health_in_interview': ['No', 'Maybe', 'Yes'],\n",
    "    'mental_health_in_interview': ['No', 'Maybe', 'Yes'],\n",
    "    'mh_hurt_career': ['No', 'Maybe', 'Yes'],\n",
    "    'mh_viewed_negatively': ['No', 'Maybe','Yes'],\n",
    "    'mh_share_friends_family': ['No', 'Maybe', 'Yes'],\n",
    "    'unsupportive_response': ['No', 'Maybe', 'Yes'],\n",
    "    'observed_mh_discussion_effect': ['No', 'Maybe', 'Yes'],\n",
    "    'family_history_mh': ['No', 'Yes'],\n",
    "    'past_mh_disorder': ['No', 'Maybe', 'Yes'],\n",
    "    'current_mh_disorder': ['No', 'Maybe', 'Yes'],\n",
    "    'diagnosed_mh_condition': ['No', 'Yes'],\n",
    "    'mh_treatment_effective': ['Never', 'Rarely', 'Sometimes', 'Often'],\n",
    "    'mh_treatment_ineffective': ['Never', 'Rarely', 'Sometimes', 'Often'],\n",
    "    'age': ['18-30', '30-40', '40-50', '50-60', '60+'],\n",
    "    'gender': ['male', 'female', 'queer'],\n",
    "    'work_remote': ['Never', 'Sometimes', 'Always']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if ordinal mapping is correct\n",
    "for col in study_data.columns:\n",
    "    for val in study_data[col].unique():\n",
    "        if col in ordinal_mapping and val not in ordinal_mapping[col]:\n",
    "            print(col, '-->', val)\n",
    "\n",
    "\n",
    "# Check if all ordinal columns are in the ordinal mapping\n",
    "for col in ordinal_cols:\n",
    "    if col not in ordinal_mapping:\n",
    "        print(col)\n",
    "\n",
    "\n",
    "# Check if all categorical columns are in the ordinal mapping\n",
    "check_list = numerical_cols + ordinal_cols + categorical_cols\n",
    "for col in study_data.columns:\n",
    "    if col not in check_list:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a copy of the preprqocessed data\n",
    "data = study_data.copy()\n",
    "\n",
    "# Ordinal Encoder for specific columns\n",
    "ordinal_encoder = OrdinalEncoder(categories=[ordinal_mapping[col] for col in ordinal_cols])\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numerical_cols),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_cols),\n",
    "        ('ord', ordinal_encoder, ordinal_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit-transform the data\n",
    "processed_data = preprocessor.fit_transform(data)\n",
    "\n",
    "# Resulting processed data is ready for clustering\n",
    "print(processed_data.shape)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "processed_data = scaler.fit_transform(processed_data)\n",
    "\n",
    "# Convert data to DataFrame\n",
    "processed_data = pd.DataFrame(processed_data, columns=preprocessor.get_feature_names_out())\n",
    "\n",
    "\n",
    "# Save encoded data to a new CSV file\n",
    "processed_data.to_csv('data/encoded_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing the dimensionality with t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_data = tsne.fit_transform(processed_data)\n",
    "\n",
    "# Visualize with labels\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(tsne_data[:, 0], tsne_data[:, 1])\n",
    "plt.title('t-SNE Visualization with Labels')\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "plt.savefig(f'data/plot_images/Vizualize_tSNE_2D.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reducing dimensionality using UMAP\n",
    "umap_model = UMAP(n_components=2)\n",
    "umap_data = umap_model.fit_transform(processed_data)\n",
    "\n",
    "# Visualize with labels\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(umap_data[:, 0], umap_data[:, 1])\n",
    "plt.title('UMAP Visualization with Labels')\n",
    "plt.xlabel('UMAP 1')\n",
    "plt.ylabel('UMAP 2')\n",
    "plt.savefig(f'data/plot_images/Vizualize_UMAP_2D.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot an ellbow curve to find the optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the data for clustering\n",
    "kmeans_data = umap_data.copy()\n",
    "\n",
    "# Determine the optimal number of clusters using the Elbow Method\n",
    "inertia = []\n",
    "k_values = range(1, 8) \n",
    "\n",
    "# Fit KMeans for each value of k\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)  # Initialize KMeans with k clusters\n",
    "    kmeans.fit(kmeans_data)                         # Fit the model\n",
    "    inertia.append(kmeans.inertia_)                 # Append the inertia to the list\n",
    "\n",
    "# Plot the Elbow curve\n",
    "plt.plot(k_values, inertia, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.savefig(f'data/plot_images/KMeans_elbow_curve.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform K-Means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal number of clusters from the Elbow Method\n",
    "optimal_k = 4\n",
    "\n",
    "# Fit KMeans with the optimal number of clusters\n",
    "kmeans = KMeans(n_clusters=optimal_k, n_init=5000, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(kmeans_data)\n",
    "\n",
    "# Remap the encoded columns to their original values\n",
    "kmeans_data = pd.DataFrame(kmeans_data)\n",
    "\n",
    "# Attach the cluster labels to the original data\n",
    "kmeans_data['Cluster'] = cluster_labels\n",
    "\n",
    "# Save the data with clusters for further exploration\n",
    "kmeans_save_data = processed_data.copy()                               # Create a copy of the processed data\n",
    "kmeans_save_data = scaler.inverse_transform(kmeans_save_data)                 # Inverse transform the scaled data\n",
    "kmeans_save_data = np.round(kmeans_save_data, 2)                              # Round the values to 2 decimal places\n",
    "kmeans_save_data = pd.DataFrame(kmeans_save_data, columns=preprocessor.get_feature_names_out())   # Convert to DataFrame\n",
    "kmeans_save_data['Cluster'] = cluster_labels                           # Add the cluster labels\n",
    "kmeans_save_data.to_csv('data/kmeans_data.csv', index=False)           # Save the data to a new CSV file\n",
    "\n",
    "# Calculate the silhouette scores for each sample\n",
    "silhouette_vals = silhouette_samples(kmeans_data, cluster_labels)\n",
    "\n",
    "# Get the average silhouette score\n",
    "avg_score = silhouette_score(kmeans_data, cluster_labels)\n",
    "\n",
    "# Number of clusters\n",
    "n_clusters = len(np.unique(cluster_labels))\n",
    "\n",
    "# Create a silhouette plot \n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "y_lower = 10  # Starting position for silhouette plots\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    # Get silhouette scores for samples in this cluster\n",
    "    cluster_silhouette_vals = silhouette_vals[cluster_labels == i]\n",
    "    cluster_silhouette_vals.sort()  # Sort values for cleaner plotting\n",
    "    \n",
    "    # Determine the y_upper position\n",
    "    y_upper = y_lower + len(cluster_silhouette_vals)\n",
    "    \n",
    "    # Fill the silhouette scores for this cluster\n",
    "    ax.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_silhouette_vals, alpha=0.7, label=f'Cluster {i}')\n",
    "    ax.text(-0.05, y_lower + 0.5 * len(cluster_silhouette_vals), str(i))  # Add cluster label\n",
    "    y_lower = y_upper + 10  # 10 for spacing between clusters\n",
    "\n",
    "# Draw a vertical line for the average silhouette score\n",
    "ax.axvline(x=avg_score, color='red', linestyle='--', label=f'Average Silhouette Score ({avg_score:.2f})')\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Silhouette Coefficient')\n",
    "ax.set_ylabel('Cluster')\n",
    "ax.set_title('Silhouette Plot for K-Means Clustering')\n",
    "ax.legend(loc='best')\n",
    "plt.savefig(f'data/plot_images/KMeans_Silouette_score.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 2D Visualise clusters\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(x=umap_data[:, 0], y=umap_data[:, 1], hue=cluster_labels, palette='viridis', s=100, alpha=0.7)\n",
    "plt.xlabel('UMAP Component 1')\n",
    "plt.ylabel('UMAP Component 2')\n",
    "plt.title('K-Means Clusters on 2D UMAP')\n",
    "plt.legend(title='Cluster')\n",
    "plt.savefig(f'data/plot_images/KMeans_UMAP_2D.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian mixture models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a Elbow Curve (BIC/AIC) to get the optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original data\n",
    "gmm_data = umap_data.copy()\n",
    "\n",
    "# Determine the optimal number of clusters using BIC and AIC\n",
    "bic = []\n",
    "aic = []\n",
    "n_components_range = range(1, 11)  # Test 1 to 10 clusters\n",
    "\n",
    "# Fit GMM with different numbers of components\n",
    "for n in n_components_range:\n",
    "    gmm = GaussianMixture(n_components=n, n_init=200, random_state=42)\n",
    "    gmm.fit(gmm_data)\n",
    "    bic.append(gmm.bic(gmm_data))   # Calculate BIC\n",
    "    aic.append(gmm.aic(gmm_data))   # Calculate AIC\n",
    "\n",
    "# Plot BIC and AIC\n",
    "plt.plot(n_components_range, bic, label='BIC')\n",
    "plt.plot(n_components_range, aic, label='AIC')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Score')\n",
    "plt.title('BIC and AIC to Determine Optimal Clusters')\n",
    "plt.legend()\n",
    "plt.savefig(f'data/plot_images/GMM_BIC_AIC_curve.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform GMM Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the optimal number of clusters from the BIC/AIC plot\n",
    "optimal_n_components = 6\n",
    "\n",
    "# Fit the GMM\n",
    "gmm = GaussianMixture(n_components=optimal_n_components,n_init= 100, random_state=42, covariance_type='full')\n",
    "gmm.fit(gmm_data)\n",
    "\n",
    "# Predict cluster labels\n",
    "cluster_labels = gmm.predict(gmm_data)\n",
    "\n",
    "# Get cluster probabilities\n",
    "cluster_probabilities = gmm.predict_proba(gmm_data)\n",
    "\n",
    "# Transform to dataframe\n",
    "gmm_data = pd.DataFrame(gmm_data)\n",
    "\n",
    "# Attach cluster labels to the original data\n",
    "gmm_data['Cluster'] = cluster_labels\n",
    "\n",
    "# Save the data with clusters for further exploration\n",
    "gmm_save_data = processed_data.copy()                               # Create a copy of the processed data\n",
    "gmm_save_data = scaler.inverse_transform(gmm_save_data)                 # Inverse transform the scaled data\n",
    "gmm_save_data = np.round(gmm_save_data, 2)                              # Round the values to 2 decimal places\n",
    "gmm_save_data = pd.DataFrame(gmm_save_data, columns=preprocessor.get_feature_names_out())   # Convert to DataFrame\n",
    "gmm_save_data['Cluster'] = cluster_labels                           # Add the cluster labels\n",
    "gmm_save_data.to_csv('data/kmeans_data.csv', index=False)           # Save the data to a new CSV file\n",
    "\n",
    "# Calculate the silhouette scores for each sample\n",
    "silhouette_vals = silhouette_samples(gmm_data, cluster_labels)\n",
    "\n",
    "# Get the average silhouette score\n",
    "avg_score = silhouette_score(gmm_data, cluster_labels)\n",
    "\n",
    "# Number of clusters\n",
    "n_clusters = len(np.unique(cluster_labels))\n",
    "\n",
    "# Create a silhouette plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "y_lower = 10  # Starting position for silhouette plots\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    # Get silhouette scores for samples in this cluster\n",
    "    cluster_silhouette_vals = silhouette_vals[cluster_labels == i]\n",
    "    cluster_silhouette_vals.sort()  # Sort values for cleaner plotting\n",
    "    \n",
    "    # Determine the y_upper position\n",
    "    y_upper = y_lower + len(cluster_silhouette_vals)\n",
    "    \n",
    "    # Fill the silhouette scores for this cluster\n",
    "    ax.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_silhouette_vals, alpha=0.7, label=f'Cluster {i}')\n",
    "    ax.text(-0.05, y_lower + 0.5 * len(cluster_silhouette_vals), str(i))  # Add cluster label\n",
    "    y_lower = y_upper + 10  # 10 for spacing between clusters\n",
    "\n",
    "# Draw a vertical line for the average silhouette score\n",
    "ax.axvline(x=avg_score, color='red', linestyle='--', label=f'Average Silhouette Score ({avg_score:.2f})')\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Silhouette Coefficient')\n",
    "ax.set_ylabel('Cluster')\n",
    "ax.set_title('Silhouette Plot for GMM Clustering')\n",
    "ax.legend(loc='best')\n",
    "plt.savefig(f'data/plot_images/GMM_Silhouette_score.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 2D Visualise clusters\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(x=umap_data[:, 0], y=umap_data[:, 1], hue=cluster_labels, palette='viridis', s=100, alpha=0.7)\n",
    "plt.xlabel('UMAP Component 1')\n",
    "plt.ylabel('UMAP Component 2')\n",
    "plt.title('GMM Clusters on 2D UMAP')\n",
    "plt.legend(title='Cluster')\n",
    "plt.savefig(f'data/plot_images/GMM_UMAP_2D.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical analysis of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the data for analysis\n",
    "stats_analysis_data = study_data.copy()\n",
    "\n",
    "# Name all columns related to mental health illness\n",
    "mental_health_cols = [\n",
    "    'past_mh_disorder',\n",
    "    'current_mh_disorder',\n",
    "]\n",
    "\n",
    "# Create a binary column to store positive answers\n",
    "stats_analysis_data['positive_mental_health'] = None\n",
    "\n",
    "# Iterate over the rows and check if there is a positive answer\n",
    "for index, row in stats_analysis_data.iterrows():\n",
    "    positive_mental_health = any(row[col] == 'Yes' for col in mental_health_cols)\n",
    "    stats_analysis_data.at[index, 'positive_mental_health'] = positive_mental_health\n",
    "\n",
    "# Create a list of columns to analyze\n",
    "columns = [\n",
    "    'self_employed',\n",
    "    'num_employees',\n",
    "    'employer_tech_company',\n",
    "    'mental_health_benefits',\n",
    "    'know_mental_health_options',\n",
    "    'employer_mh_resources',\n",
    "    'family_history_mh',\n",
    "    'gender',\n",
    "    'work_position',\n",
    "    'work_remote'\n",
    "]\n",
    "\n",
    "# Calculate if mental health issues are overrepresented in relevant columns\n",
    "for col in columns:\n",
    "    unique_values = stats_analysis_data[col].value_counts()\n",
    "\n",
    "    # Create a Series to store the results\n",
    "    results = pd.Series()\n",
    "    for value in unique_values.index:\n",
    "        total = unique_values[value]\n",
    "        positive = stats_analysis_data[stats_analysis_data[col] == value]['positive_mental_health'].sum()\n",
    "        percentage = (positive / total) * 100\n",
    "        results[value] = percentage\n",
    "\n",
    "    # Sort the results\n",
    "    results = results.sort_values(ascending=True)\n",
    "    unique_values = unique_values.sort_values(ascending=True)\n",
    "\n",
    "    # Set up the figure with two subplots side by side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # 1 row, 2 columns\n",
    "\n",
    "    # First plot\n",
    "    sns.barplot(ax=axes[0], x=results.index, y=results.values, hue=results.values, palette='viridis', legend=False)\n",
    "    axes[0].set_title(f'Percentage of MH issues in {col}')\n",
    "    axes[0].set_ylabel('Percentage')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Second plot\n",
    "    sns.barplot(ax=axes[1], x=unique_values.index, y=unique_values.values, hue=unique_values.values, palette='viridis', legend=False)\n",
    "    axes[1].set_title(f'Participants per category in {col}')\n",
    "    axes[1].set_ylabel('Percentage')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Adjust layout for better display\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f'data/plot_images/Stats_{col}.png', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster analysis K-Means Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the data for analysis\n",
    "plot_cluster_data = kmeans_save_data.copy()\n",
    "\n",
    "# Compute mean of each feature for each cluster\n",
    "cluster_centers = plot_cluster_data.groupby('Cluster').mean()\n",
    "\n",
    "# Get absolute values to identify the most influential features\n",
    "absolute_cluster_centers = cluster_centers.abs()\n",
    "\n",
    "# Identify top 5 important features per cluster\n",
    "important_features_per_cluster = absolute_cluster_centers.apply(lambda x: x.nlargest(5).index.tolist(), axis=1)\n",
    "\n",
    "# Visualize the important features per cluster\n",
    "for cluster, features in important_features_per_cluster.items():\n",
    "\n",
    "    # Get the top features for this cluster\n",
    "    top_features = cluster_centers.loc[cluster, features]\n",
    "    \n",
    "    # Plot the barplot for these features\n",
    "    ax = sns.barplot(x=top_features.index, y=top_features.values, hue=top_features.values, palette='viridis', legend=False)\n",
    "    plt.title(f'Cluster {cluster} - Important Features')\n",
    "    plt.ylabel('Mean Value')\n",
    "    plt.xlabel('Feature')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Add mode values as annotations inside the bars\n",
    "    for i, feature in enumerate(top_features.index):\n",
    "        \n",
    "        # Get the mode value for this feature in this cluster\n",
    "        mode_value = plot_cluster_data[plot_cluster_data['Cluster'] == cluster][feature].mode().iloc[0]\n",
    "\n",
    "        # Map the numeric mode values back to the original values. Remove 'ord__' prefix\n",
    "        mode_value = ordinal_mapping[feature.replace('ord__', '')][int(mode_value)]\n",
    "\n",
    "        # Add the mode value as an annotation inside the bar\n",
    "        bar_value = top_features.values[i]  # Bar height (mean value)\n",
    "        ax.text(\n",
    "            x=i,  # X-coordinate (bar index)\n",
    "            y=bar_value / 2,  # Y-coordinate (middle of the bar)\n",
    "            s=mode_value,  # Text displaying mode value\n",
    "            ha='center',  # Horizontal alignment\n",
    "            va='center',  # Vertical alignment\n",
    "            fontsize=10,  # Font size\n",
    "            color='white'  # Text color for readability\n",
    "        )\n",
    "\n",
    "    # Get the number of MH issues for this cluster\n",
    "    current_mh_issues = 0\n",
    "    past_mh_issues = 0\n",
    "    total_mh_issues = 0\n",
    "    for index, row in plot_cluster_data[plot_cluster_data['Cluster'] == cluster].iterrows():\n",
    "        if row['ord__current_mh_disorder'] == 2:\n",
    "            current_mh_issues += 1\n",
    "        elif row['ord__past_mh_disorder'] == 2:\n",
    "            past_mh_issues += 1\n",
    "        if row['ord__current_mh_disorder'] == 2 or row['ord__past_mh_disorder'] == 2:\n",
    "            total_mh_issues += 1\n",
    "\n",
    "    # Get the number of participants in this cluster\n",
    "    participants = plot_cluster_data['Cluster'].value_counts().get(cluster, 0)\n",
    "\n",
    "    # Calculate the percentage of participants with MH issues\n",
    "    current_mh_issues_percentage = (current_mh_issues / participants) * 100\n",
    "    past_mh_issues_percentage = (past_mh_issues / participants) * 100\n",
    "    total_mh_issues_percentage = (total_mh_issues / participants) * 100\n",
    "\n",
    "    # Add the percentage and participant count as a description at the bottom\n",
    "    plt.figtext(\n",
    "        0,  # X-coordinate (center of the figure)\n",
    "        -0.5,  # Y-coordinate (below the plot)\n",
    "        f'''Cluster {cluster}\n",
    "            Current/Past MH Issues: {current_mh_issues_percentage:.2f}% / {past_mh_issues_percentage:.2f}%\n",
    "            Total MH Issues: {total_mh_issues_percentage:.2f}%\n",
    "            Participants: {participants}''',\n",
    "        wrap=True,  # Wrap text if too long\n",
    "        ha='left',  # Horizontal alignment\n",
    "        fontsize=10,  # Font size\n",
    "        color='black',  # Text color\n",
    "    )\n",
    "    plt.savefig(f'data/plot_images/KMeans_Cluster_{cluster}_Important_Features.png', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster analysis GMM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the data for analysis\n",
    "plot_cluster_data = gmm_save_data.copy()\n",
    "\n",
    "# Compute mean of each feature for each cluster\n",
    "cluster_centers = plot_cluster_data.groupby('Cluster').mean()\n",
    "\n",
    "# Get absolute values to identify the most influential features\n",
    "absolute_cluster_centers = cluster_centers.abs()\n",
    "\n",
    "# Identify top 5 important features per cluster\n",
    "important_features_per_cluster = absolute_cluster_centers.apply(lambda x: x.nlargest(5).index.tolist(), axis=1)\n",
    "\n",
    "# Visualize the important features per cluster\n",
    "for cluster, features in important_features_per_cluster.items():\n",
    "\n",
    "    # Get the top features for this cluster\n",
    "    top_features = cluster_centers.loc[cluster, features]\n",
    "    \n",
    "    # Plot the barplot for these features\n",
    "    ax = sns.barplot(x=top_features.index, y=top_features.values, hue=top_features.values, palette='viridis', legend=False)\n",
    "    plt.title(f'Cluster {cluster} - Important Features')\n",
    "    plt.ylabel('Mean Value')\n",
    "    plt.xlabel('Feature')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Add mode values as annotations inside the bars\n",
    "    for i, feature in enumerate(top_features.index):\n",
    "        \n",
    "        # Get the mode value for this feature in this cluster\n",
    "        mode_value = plot_cluster_data[plot_cluster_data['Cluster'] == cluster][feature].mode().iloc[0]\n",
    "\n",
    "        # Map the numeric mode values back to the original values. Remove 'ord__' prefix\n",
    "        mode_value = ordinal_mapping[feature.replace('ord__', '')][int(mode_value)]\n",
    "\n",
    "        # Add the mode value as an annotation inside the bar\n",
    "        bar_value = top_features.values[i]  # Bar height (mean value)\n",
    "        ax.text(\n",
    "            x=i,  # X-coordinate (bar index)\n",
    "            y=bar_value / 2,  # Y-coordinate (middle of the bar)\n",
    "            s=mode_value,  # Text displaying mode value\n",
    "            ha='center',  # Horizontal alignment\n",
    "            va='center',  # Vertical alignment\n",
    "            fontsize=10,  # Font size\n",
    "            color='white'  # Text color for readability\n",
    "        )\n",
    "\n",
    "    # Get the number of MH issues for this cluster\n",
    "    current_mh_issues = 0\n",
    "    past_mh_issues = 0\n",
    "    total_mh_issues = 0\n",
    "    for index, row in plot_cluster_data[plot_cluster_data['Cluster'] == cluster].iterrows():\n",
    "        if row['ord__current_mh_disorder'] == 2:\n",
    "            current_mh_issues += 1\n",
    "        elif row['ord__past_mh_disorder'] == 2:\n",
    "            past_mh_issues += 1\n",
    "        if row['ord__current_mh_disorder'] == 2 or row['ord__past_mh_disorder'] == 2:\n",
    "            total_mh_issues += 1\n",
    "\n",
    "    # Get the number of participants in this cluster\n",
    "    participants = plot_cluster_data['Cluster'].value_counts().get(cluster, 0)\n",
    "\n",
    "    # Calculate the percentage of participants with MH issues\n",
    "    current_mh_issues_percentage = (current_mh_issues / participants) * 100\n",
    "    past_mh_issues_percentage = (past_mh_issues / participants) * 100\n",
    "    total_mh_issues_percentage = (total_mh_issues / participants) * 100\n",
    "\n",
    "    # Add the percentage and participant count as a description at the bottom\n",
    "    plt.figtext(\n",
    "        0,  # X-coordinate (center of the figure)\n",
    "        -0.5,  # Y-coordinate (below the plot)\n",
    "        f'''Cluster {cluster}\n",
    "            Current/Past MH Issues: {current_mh_issues_percentage:.2f}% / {past_mh_issues_percentage:.2f}%\n",
    "            Total MH Issues: {total_mh_issues_percentage:.2f}%\n",
    "            Participants: {participants}''',\n",
    "        wrap=True,  # Wrap text if too long\n",
    "        ha='left',  # Horizontal alignment\n",
    "        fontsize=10,  # Font size\n",
    "        color='black',  # Text color\n",
    "    )\n",
    "\n",
    "    plt.savefig(f'data/plot_images/GMM_Cluster_{cluster}_Important_Features.png', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
